{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 22:38:00.453141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-24 22:38:00.453165: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spektral as spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abidata = '/root/digi_twin_summer_wireless_dataset/data_split_clean'\n",
    "abidata2 = '/root/digi_twin_summer_wireless_dataset/dataset-2'\n",
    "kpi_ = '/*kpis.txt'\n",
    "tfc_ = '/*traffic.txt'\n",
    "traincon = '/train'\n",
    "valcon = '/validate'\n",
    "testcon = '/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainkpis = glob.glob(abidata+traincon+kpi_)\n",
    "trainkpis.sort()\n",
    "traintraffics = glob.glob(abidata+traincon+tfc_)\n",
    "traintraffics.sort()\n",
    "\n",
    "valkpis = glob.glob(abidata+valcon+kpi_)\n",
    "valkpis.sort()\n",
    "valtraffics = glob.glob(abidata+valcon+tfc_)\n",
    "valtraffics.sort()\n",
    "\n",
    "testkpis = glob.glob(abidata+testcon+kpi_)\n",
    "testkpis.sort()\n",
    "testtraffics = glob.glob(abidata+testcon+tfc_)\n",
    "testtraffics.sort()\n",
    "\n",
    "test2kpis = glob.glob(abidata2+testcon+kpi_)\n",
    "test2kpis.sort()\n",
    "test2traffics = glob.glob(abidata2+testcon+tfc_)\n",
    "test2traffics.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = [[0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
    " [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
    " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
    " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
    " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
    " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]]\n",
    "lap = spk.utils.convolution.gcn_filter(np.array(adj_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnoDataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, filenames, hparams):\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        kpis, tris = filenames\n",
    "        \n",
    "        \n",
    "        kpiframe = pd.read_csv(kpis[0], header=None)\n",
    "        # for k in kpis[1:]:\n",
    "        #     kpiframe = pd.concat([kpiframe, pd.read_csv(k, header=None)])\n",
    "        self.kpiframe = kpiframe.reset_index(drop=True)\n",
    "\n",
    "        self.n = kpiframe.shape[0]\n",
    "        triframe = pd.read_csv(tris[0], header=None)\n",
    "        # for k in tris[1:]:\n",
    "        #     triframe = pd.concat([triframe, pd.read_csv(k, header=None)])\n",
    "        self.triframe = triframe.reset_index(drop=True)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index=None):\n",
    "        \n",
    "        a = self.triframe.loc[index]\n",
    "        b = self.kpiframe.loc[index]\n",
    "        #links and paths\n",
    "        f_n_links = 42\n",
    "        f_n_nodes = 14\n",
    "        f_n_paths = 10\n",
    "\n",
    "        f_degrees = [1.0]*14\n",
    "        \n",
    "        f_capacities = [1.0]*42\n",
    "\n",
    "        traffic_in = [float(a[i]) for i in range(0, len(a), 2)]\n",
    "        traffic_ot = [float(a[i]) for i in range(1, len(a), 2)]\n",
    "        f_traffic = [traffic_in, traffic_ot]\n",
    "\n",
    "        f_links_to_paths = [0, 5, 5, 14, 34, 11, 18, 6, 25, 20, 38, 33, 36, 40, 28, 30, 16, 19]\n",
    "\n",
    "        f_link_state_dim = 32\n",
    "\n",
    "        f_nodes_to_paths = [0, 1, 1, 7, 10, 2, 5, 3, 6, 4, 9, 10, 9, 11, 12, 13, 5, 4]\n",
    "\n",
    "        f_node_state_dim = 32\n",
    "\n",
    "        f_links_to_nodes = [0, 0, 0, 1, 1, 1, 3, 3, 3, 2, 2, 2, 7, 7, 7, 5, 5, 5, 5, 4, 4, 4, 8, 8, 8, 6, 6, 12, 12, 12, 13, 13, 10, 10, 10, 10, 9, 9, 9, 11, 11, 11]\n",
    "\n",
    "        f_nodes_to_links = [0, 1, 2, 3, 4, 5, 9, 10, 11, 6, 7, 8, 19, 20, 21, 15, 16, 17, 18, 25, 26, 12, 13, 14, 22, 23, 24, 36, 37, 38, 32, 33, 34, 35, 39, 40, 41, 27, 28, 29, 30, 31]\n",
    "\n",
    "        f_paths_to_x = [0, 0, 1, 1, 1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 8, 9, 9, 9]\n",
    "        f_sequences_paths_x = [0, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2]\n",
    "\n",
    "        l_delay = [float(b[i]) for i in range(2, len(b), 3)]\n",
    "\n",
    "        # features = {\n",
    "        #     'n_nodes': tf.Variable(tf.constant(f_n_nodes)), #0\n",
    "        #     'n_links': tf.Variable(tf.constant(f_n_links)), #1\n",
    "        #     'n_paths': tf.Variable(tf.constant(f_n_paths)), #2\n",
    "        #     'node_init': tf.Variable(tf.constant(f_degrees)), #3\n",
    "        #     'link_init': tf.Variable(tf.constant(f_capacities)), #4\n",
    "        #     'path_init': tf.Variable(tf.constant(f_traffic)), #5\n",
    "        #     'nodes_to_paths': tf.Variable(tf.constant(f_nodes_to_paths)), #6\n",
    "        #     'links_to_paths': tf.Variable(tf.constant(f_links_to_paths)), #7\n",
    "        #     'links_to_nodes': tf.Variable(tf.constant(f_links_to_nodes)), #8\n",
    "        #     'paths_to_x': tf.Variable(tf.constant(f_paths_to_x)), #9\n",
    "        #     'sequences_paths_x': tf.Variable(tf.constant(f_sequences_paths_x)), #10\n",
    "        #     'node_lapacian': tf.Variable(tf.constant(lap, dtype=np.float32)) #11\n",
    "        # }\n",
    "        features = [\n",
    "            tf.Variable(tf.constant(f_degrees)), #0\n",
    "            tf.Variable(tf.constant(f_capacities)), #1\n",
    "            tf.Variable(tf.constant(f_traffic)), #2\n",
    "            tf.Variable(tf.constant(f_n_nodes)), #3\n",
    "            tf.Variable(tf.constant(f_n_links)), #4\n",
    "            tf.Variable(tf.constant(f_n_paths)), #5\n",
    "            tf.Variable(tf.constant(f_nodes_to_paths)), #6\n",
    "            tf.Variable(tf.constant(f_links_to_paths)), #7\n",
    "            tf.Variable(tf.constant(f_links_to_nodes)), #8\n",
    "            tf.Variable(tf.constant(f_paths_to_x)), #9\n",
    "            tf.Variable(tf.constant(f_sequences_paths_x)), #10\n",
    "            tf.Variable(tf.constant(adj_mat, dtype=np.float32)), #11\n",
    "            tf.Variable(tf.constant(f_nodes_to_links)), #12\n",
    "\n",
    "        ]\n",
    "\n",
    "        labels = [\n",
    "            tf.Variable(tf.constant(l_delay)), #0\n",
    "        ]\n",
    "\n",
    "        #return sample\n",
    "        return [features, labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n #// self.hparams['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'node_count':14,\n",
    "    'link_state_dim':32, \n",
    "    #[4, 8, 16, 32, 64]\n",
    "    'path_state_dim':32,\n",
    "    #[2, 4, 8, 16, 32, 64]\n",
    "    'node_state_dim':32,\n",
    "    #[2, 4, 8, 16, 32, 64]\n",
    "    'T':8,\n",
    "    'readout_units':8,\n",
    "    'learning_rate':0.001,\n",
    "    #[.001, .01, .05]\n",
    "    'batch_size':10,\n",
    "    #[8, 16, 32, 64]\n",
    "    'dropout_rate':0.5,\n",
    "    #[.5] leave\n",
    "    'l2':0.1,\n",
    "    #regulirization constants\n",
    "    #[.05, .1, .2]\n",
    "    'l2_2':0.01,\n",
    "    #[.005, .01, .02]\n",
    "    'learn_embedding':True, # If false, only the readout is trained\n",
    "    'readout_layers':2, # number of hidden layers in readout model\n",
    "    #[2, 3, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = UnoDataGen((trainkpis, traintraffics), hparams=hparams)\n",
    "\n",
    "valgen = UnoDataGen((valkpis, valtraffics), hparams=hparams)\n",
    "\n",
    "testgen = UnoDataGen((testkpis, testtraffics), hparams=hparams)\n",
    "\n",
    "test2gen = UnoDataGen((test2kpis, test2traffics), hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algo1(tf.keras.Model):\n",
    "    def __init__(self,hparams, output_units=1, final_activation=None):\n",
    "        super(Algo1, self).__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        self.output_units = output_units\n",
    "        self.final_activation = final_activation\n",
    "        \n",
    "\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        del input_shape\n",
    "\n",
    "        #state updaters\n",
    "        # self.edge_update = tf.keras.layers.GRUCell(hparams['link_state_dim'], name=\"edge_update\")\n",
    "        self.path_update = tf.keras.layers.GRUCell(hparams['path_state_dim'], name=\"path_update\")\n",
    "        self.node_update = spk.layers.ECCConv(hparams['node_state_dim'])\n",
    "\n",
    "        self.edge_update = tf.keras.models.Sequential(name=\"edge_update\")\n",
    "        for i in range(5):\n",
    "            self.edge_update.add(tf.keras.layers.Dense(hparams['link_state_dim']))\n",
    "\n",
    "        #readout-final\n",
    "        \n",
    "        self.readout = tf.keras.models.Sequential(name='readout')\n",
    "\n",
    "        for i in range(hparams['readout_layers']):\n",
    "            self.readout.add(tf.keras.layers.Dense(hparams['readout_units'], \n",
    "                    activation=tf.nn.selu,\n",
    "                    kernel_regularizer=tf.keras.regularizers.L2(hparams['l2'])))\n",
    "\n",
    "            self.readout.add(tf.keras.layers.Dropout(rate=hparams['dropout_rate']))\n",
    "\n",
    "        self.final = tf.keras.layers.Dense(self.output_units, \n",
    "                kernel_regularizer=tf.keras.regularizers.L2(hparams['l2_2']),\n",
    "                activation = self.final_activation )\n",
    "        \n",
    "        # self.edge_update.build(tf.TensorShape([None,hparams['path_state_dim']]))\n",
    "        self.path_update.build(tf.TensorShape([None,hparams['link_state_dim']+hparams['node_state_dim']]))\n",
    "\n",
    "        self.edge_update.build(tf.TensorShape([None,hparams['link_state_dim']+hparams['node_state_dim']+hparams['path_state_dim']]))\n",
    "\n",
    "        self.readout.build(input_shape = [None,hparams['path_state_dim']])\n",
    "        self.final.build(input_shape = [None,hparams['path_state_dim'] + hparams['readout_units'] ])\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        #call == v ==\n",
    "        f_ = inputs\n",
    "\n",
    "        #state init\n",
    "        # shape = tf.stack([f_['n_links'],hparams['link_state_dim']-1], axis=0)\n",
    "        shape = tf.stack([f_[4],hparams['link_state_dim']-1], axis=0)\n",
    "        link_state = tf.concat([\n",
    "            # tf.expand_dims(f_['link_init'],axis=1),\n",
    "            tf.expand_dims(f_[1],axis=1),\n",
    "            tf.zeros(shape)\n",
    "        ], axis=1)\n",
    "\n",
    "        # shape = tf.stack([f_['n_nodes'],hparams['node_state_dim']-1], axis=0)\n",
    "        shape = tf.stack([f_[3],hparams['node_state_dim']-1], axis=0)\n",
    "        node_state = tf.concat([\n",
    "            # tf.expand_dims(f_['node_init'],axis=1),\n",
    "            tf.expand_dims(f_[0],axis=1),\n",
    "            tf.zeros(shape)\n",
    "        ], axis=1)\n",
    "\n",
    "        # shape = tf.stack([f_['n_paths'],hparams['path_state_dim']-1], axis=0)\n",
    "        shape = tf.stack([f_[5],hparams['path_state_dim']-2], axis=0)\n",
    "        path_state = tf.concat([\n",
    "            tf.expand_dims(f_[2][0],axis=1),\n",
    "            tf.expand_dims(f_[2][1],axis=1),\n",
    "            # tf.expand_dims(f_['path_init'],axis=1),\n",
    "            # tf.expand_dims(f_[5],axis=1),\n",
    "            tf.zeros(shape)\n",
    "        ], axis=1)\n",
    "\n",
    "        #pull for both\n",
    "        # paths = f_['paths_to_x']\n",
    "        paths = f_[9]\n",
    "        # seqs=  f_['sequences_paths_x']\n",
    "        seqs=  f_[10]\n",
    "        # n_paths = f_['n_paths']\n",
    "        n_paths = f_[5]\n",
    "        \n",
    "        for _ in range(hparams['T']):\n",
    "        #stuff for both\n",
    "            ids=tf.stack([paths, seqs], axis=1)\n",
    "            max_len = tf.reduce_max(seqs)+1\n",
    "            lens = tf.math.segment_sum(data=tf.ones_like(paths),\n",
    "                                    segment_ids=paths)\n",
    "\n",
    "            #link stuff\n",
    "            # h_ = tf.gather(link_state,f_['links_to_paths'])\n",
    "            h_ = tf.gather(link_state,f_[7])\n",
    "\n",
    "            shape = tf.stack([n_paths, max_len, hparams['link_state_dim']])\n",
    "            link_inputs = tf.scatter_nd(ids, h_, shape)\n",
    "            \n",
    "            #node stuff\n",
    "            # h_ = tf.gather(link_state,f_['nodes_to_paths'])\n",
    "            h_ = tf.gather(link_state,f_[6])\n",
    "\n",
    "            shape = tf.stack([n_paths, max_len, hparams['node_state_dim']])\n",
    "            node_inputs = tf.scatter_nd(ids, h_, shape)\n",
    "            \n",
    "            x_inputs = tf.concat([link_inputs, node_inputs], axis=2)\n",
    "            \n",
    "            #updating path_state\n",
    "            outputs, path_state = tf.compat.v1.nn.dynamic_rnn(cell = self.path_update,\n",
    "                                                inputs = x_inputs,\n",
    "                                                sequence_length = lens,\n",
    "                                                initial_state = path_state,\n",
    "                                                dtype=tf.float32)\n",
    "            \n",
    "            m = tf.gather_nd(outputs,ids)\n",
    "            # m = tf.math.unsorted_segment_sum(m, f_['links_to_paths'] ,f_['n_links'])\n",
    "            m = tf.math.unsorted_segment_sum(m, f_[7] ,f_[4])\n",
    "            #fitting nodes to links\n",
    "            # h_ = tf.gather(node_state,f_['links_to_nodes'])\n",
    "            h_ = tf.gather(node_state,f_[8])\n",
    "\n",
    "            _con = tf.concat([h_, link_state, m], axis=1)\n",
    "            link_state = self.edge_update(_con)\n",
    "\n",
    "            # link_state,_ = self.edge_update(m, [red])\n",
    "            \n",
    "            #node update\n",
    "\n",
    "            # print(tf.gather(link_state, f_[12]))\n",
    "            # node_state = self.node_update((node_state, f_['adj_mat'], link_state))\n",
    "            node_state = self.node_update((node_state, f_[11], link_state))\n",
    "\n",
    "        #readout\n",
    "                \n",
    "        if hparams['learn_embedding']:\n",
    "            r = self.readout(path_state,training=training)\n",
    "            o = self.final(tf.concat([r,path_state], axis=1))\n",
    "            \n",
    "        else:\n",
    "            r = self.readout(tf.stop_gradient(path_state),training=training)\n",
    "            o = self.final(tf.concat([r, tf.stop_gradient(path_state)], axis=1) )\n",
    "\n",
    "        return o\n",
    "\n",
    "    def train_step(self, data):\n",
    "        features, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            print(tf.math.is_nan(predictions))\n",
    "            loc  = predictions[...,0]\n",
    "            delay_prediction = loc\n",
    "            loss = tf.keras.metrics.mean_squared_error(labels[0], loc)\n",
    "            print(tf.math.is_nan(loss))\n",
    "\n",
    "            regularization_loss = sum(self.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "            \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        ret = {\n",
    "            'loss':loss,\n",
    "            'label/mean/delay':tf.math.reduce_mean(labels[0]),\n",
    "            'prediction/mean/delay': tf.math.reduce_mean(delay_prediction)\n",
    "            }\n",
    "        return ret\n",
    "\n",
    "    def test_step(self, data):\n",
    "        features, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=False)\n",
    "            loc  = predictions[...,0]\n",
    "            delay_prediction = loc\n",
    "            loss = tf.keras.metrics.mean_squared_error(labels[0], loc)\n",
    "\n",
    "            regularization_loss = sum(self.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "            \n",
    "        # gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        \n",
    "        # self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        ret = {\n",
    "            'loss':loss,\n",
    "            'label/mean/delay':tf.math.reduce_mean(labels[0]),\n",
    "            'prediction/mean/delay': tf.math.reduce_mean(delay_prediction)\n",
    "            }\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ = Algo1(hparams)\n",
    "r_.build()\n",
    "r_.compile(optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spektral/layers/convolutional/ecc_conv.py:162: UserWarning: Casting dense adjacency matrix to SparseTensor.This can be an expensive operation. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.isnan(r_(traingen.__getitem__(0)[0])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv0 = tf.keras.callbacks.CSVLogger(\n",
    "    '../../Logs/UnoNet0722', separator=',', append=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Casting dense adjacency matrix to SparseTensor.This can be an expensive operation. \n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IsNan:0\", shape=(None, 1), dtype=bool)\n",
      "Tensor(\"IsNan_1:0\", shape=(), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"concat:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_5:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_4:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_8:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_7:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_10:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_9:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_12:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_11:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_15:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_14:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_16:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_19:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_18:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_22:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_21:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_24:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_23:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_10:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_26:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_25:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_11:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_29:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_28:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_12:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_31:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_30:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_13:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_33:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_32:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_14:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_36:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_35:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_15:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_38:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_37:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_16:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_40:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_39:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_17:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_43:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_42:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_18:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/algo1_6/Reshape_45:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/algo1_6/Reshape_44:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/algo1_6/Cast_19:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IsNan:0\", shape=(None, 1), dtype=bool)\n",
      "Tensor(\"IsNan_1:0\", shape=(), dtype=bool)\n",
      "174/174 [==============================] - 10s 20ms/step - loss: 245546.1863 - label/mean/delay: 429.7069 - prediction/mean/delay: 1.7681 - val_loss: 211338.4531 - val_label/mean/delay: 389.4000 - val_prediction/mean/delay: 5.9817\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 2s 13ms/step - loss: 242099.8451 - label/mean/delay: 429.6686 - prediction/mean/delay: 14.0996 - val_loss: 195716.5312 - val_label/mean/delay: 389.4000 - val_prediction/mean/delay: 27.0336\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 2s 13ms/step - loss: nan - label/mean/delay: 429.7909 - prediction/mean/delay: nan - val_loss: nan - val_label/mean/delay: 389.4000 - val_prediction/mean/delay: nan\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 2s 13ms/step - loss: nan - label/mean/delay: 423.7343 - prediction/mean/delay: nan - val_loss: nan - val_label/mean/delay: 389.4000 - val_prediction/mean/delay: nan\n",
      "Epoch 5/20\n",
      "147/174 [========================>.....] - ETA: 0s - loss: nan - label/mean/delay: 433.0463 - prediction/mean/delay: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m loops \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m r_\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=3'>4</a>\u001b[0m     traingen, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=4'>5</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mtraingen\u001b[39m.\u001b[39;49m\u001b[39m__len__\u001b[39;49m()\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49mepochs, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=5'>6</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs\u001b[39m*\u001b[39;49mloops, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=6'>7</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalgen, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=7'>8</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalgen\u001b[39m.\u001b[39;49m\u001b[39m__len__\u001b[39;49m()\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49mepochs, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=8'>9</a>\u001b[0m     \u001b[39m# callbacks=[csv0]\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4a6f73655466326473222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6c69646f342e6563652e726963652e656475227d7d/root/code/RouteNetButBetter/Algorithym_1/algo1.ipynb#ch0000019vscode-remote?line=9'>10</a>\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "loops = 1\n",
    "r_.fit(\n",
    "    traingen, \n",
    "    steps_per_epoch=traingen.__len__()//epochs, \n",
    "    epochs=epochs*loops, \n",
    "    validation_data=valgen, \n",
    "    validation_steps=valgen.__len__()//epochs, \n",
    "    # callbacks=[csv0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.isnan(r_(testgen.__getitem__(0)[0])))[0]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nansum(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s 4ms/step - loss: nan - label/mean/delay: 428.4890 - prediction/mean/delay: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[492.1000061035156, nan, nan]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UnoNet Test1\n",
    "r_.evaluate(testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s 4ms/step - loss: 19958.6998 - label/mean/delay: 431.3978 - prediction/mean/delay: 413.7140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[497.79998779296875, 42391.21484375, 424.42315673828125]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UnoNet Tetst2\n",
    "r_.evaluate(test2gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
