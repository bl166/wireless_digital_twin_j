{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat0 = '/root/dataset/'\n",
    "litedat0 = '/root/datasetlite/'\n",
    "dat1 = '/*.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular data\n",
    "train_data = glob.glob(dat0+'train'+dat1)\n",
    "test_1_data = glob.glob(dat0+'test-1'+dat1)\n",
    "test_2_data = glob.glob(dat0+'test-2'+dat1)\n",
    "validate_data = glob.glob(dat0+'validate'+dat1)\n",
    "validate2_data = glob.glob(dat0+'validate-2'+dat1)\n",
    "\n",
    "#lite data\n",
    "lite_train_data = glob.glob(litedat0+'train'+dat1)\n",
    "lite_test_1_data = glob.glob(litedat0+'test-1'+dat1)\n",
    "lite_test_2_data = glob.glob(litedat0+'test-2'+dat1)\n",
    "lite_validate_data = glob.glob(litedat0+'validate'+dat1)\n",
    "lite_validate2_data = glob.glob(litedat0+'validate-2'+dat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abidata = '/root/abidata/digi_twin_summer_wireless_dataset/allTrials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(abidata+'/trial-1116731144_kpis.txt',header=None).loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t_sample = np.resize(pd.read_csv(abidata+'/trial-1116731144_traffic.txt',header=None).loc[0], (10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old model\n",
    "# class RouteNet(tf.keras.Model):\n",
    "#     def __init__(self,hparams, output_units=1, final_activation=None):\n",
    "#         super(RouteNet, self).__init__()\n",
    "\n",
    "#         self.hparams = hparams\n",
    "#         self.output_units = output_units\n",
    "#         self.final_activation = final_activation\n",
    "        \n",
    "\n",
    "        \n",
    "#     def build(self, input_shape=None):\n",
    "#         del input_shape\n",
    "\n",
    "#         self.edge_update = tf.keras.layers.GRUCell(self.hparams.link_state_dim, name=\"edge_update\")\n",
    "#         self.path_update = tf.keras.layers.GRUCell(self.hparams.path_state_dim, name=\"path_update\")\n",
    "        \n",
    "        \n",
    "#         self.readout = tf.keras.models.Sequential(name='readout')\n",
    "\n",
    "#         for i in range(self.hparams.readout_layers):\n",
    "#             self.readout.add(tf.keras.layers.Dense(self.hparams.readout_units, \n",
    "#                     activation=tf.nn.selu,\n",
    "#                     kernel_regularizer=tf.contrib.layers.l2_regularizer(self.hparams.l2)))\n",
    "#             self.readout.add(tf.keras.layers.Dropout(rate=self.hparams.dropout_rate))\n",
    "\n",
    "#         self.final = keras.layers.Dense(self.output_units, \n",
    "#                 kernel_regularizer=tf.contrib.layers.l2_regularizer(self.hparams.l2_2),\n",
    "#                 activation = self.final_activation )\n",
    "        \n",
    "#         self.edge_update.build(tf.TensorShape([None,self.hparams.path_state_dim]))\n",
    "#         self.path_update.build(tf.TensorShape([None,self.hparams.link_state_dim]))\n",
    "#         self.readout.build(input_shape = [None,self.hparams.path_state_dim])\n",
    "#         self.final.build(input_shape = [None,self.hparams.path_state_dim + self.hparams.readout_units ])\n",
    "\n",
    "#         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "#         self.built = True\n",
    "    \n",
    "    \n",
    "#     def call(self, inputs, training=False):\n",
    "#         '''\n",
    "\n",
    "#         outputs:\n",
    "#             Natural parameter\n",
    "        \n",
    "#         '''\n",
    "#         #this takes features\n",
    "#         f_ = inputs\n",
    "#         sample = inputs\n",
    "\n",
    "#         shape = tf.stack([f_['n_links'],self.hparams.link_state_dim-1], axis=0)\n",
    "#         #link_state = tf.zeros(shape)\n",
    "#         link_state = tf.concat([\n",
    "#             tf.expand_dims(f_['capacities'],axis=1),\n",
    "#             tf.zeros(shape)\n",
    "#         ], axis=1)\n",
    "\n",
    "#         shape = tf.stack([f_['n_paths'],self.hparams.path_state_dim-1], axis=0)\n",
    "#         path_state = tf.concat([\n",
    "#             tf.expand_dims(f_['traffic'][0:f_[\"n_paths\"]],axis=1),\n",
    "#             tf.zeros(shape)\n",
    "#         ], axis=1)\n",
    "\n",
    "#         links = f_['links']\n",
    "#         paths = f_['paths']\n",
    "#         seqs=  f_['sequences']\n",
    "        \n",
    "#         for _ in range(self.hparams.T):\n",
    "        \n",
    "#             h_ = tf.gather(link_state,links)\n",
    "\n",
    "#             #TODO move this to feature calculation\n",
    "#             ids=tf.stack([paths, seqs], axis=1)            \n",
    "#             max_len = tf.reduce_max(seqs)+1\n",
    "#             shape = tf.stack([f_['n_paths'], max_len, self.hparams.link_state_dim])\n",
    "#             lens = tf.segment_sum(data=tf.ones_like(paths),\n",
    "#                                     segment_ids=paths)\n",
    "\n",
    "#             link_inputs = tf.scatter_nd(ids, h_, shape)\n",
    "#             #TODO move to tf.keras.RNN\n",
    "#             outputs, path_state = tf.nn.dynamic_rnn(self.path_update,\n",
    "#                                                     link_inputs,\n",
    "#                                                     sequence_length=lens,\n",
    "#                                                     initial_state = path_state,\n",
    "#                                                     dtype=tf.float32)\n",
    "#             m = tf.gather_nd(outputs,ids)\n",
    "\n",
    "#             m = tf.unsorted_segment_sum(m, links ,f_['n_links'])\n",
    "\n",
    "#             #Keras cell expects a list\n",
    "#             link_state,_ = self.edge_update(m, [link_state])\n",
    "            \n",
    "#         if self.hparams.learn_embedding:\n",
    "#             r = self.readout(path_state,training=training)\n",
    "#             o = self.final(tf.concat([r,path_state], axis=1))\n",
    "            \n",
    "#         else:\n",
    "#             r = self.readout(tf.stop_gradient(path_state),training=training)\n",
    "#             o = self.final(tf.concat([r, tf.stop_gradient(path_state)], axis=1) )\n",
    "\n",
    "#         return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(tf.keras.Sequential):\n",
    "#   \"\"\"A custom sequential model that overrides `Model.train_step`.\"\"\"\n",
    "\n",
    "#   def train_step(self, data):\n",
    "#     batch_data, labels = data\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#       predictions = self(batch_data, training=True)\n",
    "#       # Compute the loss value (the loss function is configured\n",
    "#       # in `Model.compile`).\n",
    "#       loss = self.compiled_loss(labels, predictions)\n",
    "\n",
    "#     # Compute the gradients of the parameters with respect to the loss.\n",
    "#     gradients = tape.gradient(loss, self.trainable_variables)\n",
    "#     # Perform gradient descent by updating the weights/parameters.\n",
    "#     self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "#     # Update the metrics (includes the metric that tracks the loss).\n",
    "#     self.compiled_metrics.update_state(labels, predictions)\n",
    "#     # Return a dict mapping metric names to the current values.\n",
    "#     return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteNet(tf.keras.Model):\n",
    "    def __init__(self,hparams, output_units=1, final_activation=None):\n",
    "        super(RouteNet, self).__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        self.output_units = output_units\n",
    "        self.final_activation = final_activation\n",
    "        \n",
    "\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        del input_shape\n",
    "\n",
    "        self.edge_update = tf.keras.layers.GRUCell(self.hparams.link_state_dim, name=\"edge_update\")\n",
    "        self.path_update = tf.keras.layers.GRUCell(self.hparams.path_state_dim, name=\"path_update\")\n",
    "        \n",
    "        \n",
    "        self.readout = tf.keras.models.Sequential(name='readout')\n",
    "\n",
    "        for i in range(self.hparams.readout_layers):\n",
    "            self.readout.add(tf.keras.layers.Dense(self.hparams.readout_units, \n",
    "                    activation=tf.nn.selu,\n",
    "                    kernel_regularizer=tf.contrib.layers.l2_regularizer(self.hparams.l2)))\n",
    "            self.readout.add(tf.keras.layers.Dropout(rate=self.hparams.dropout_rate))\n",
    "\n",
    "        self.final = keras.layers.Dense(self.output_units, \n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.hparams.l2_2),\n",
    "                activation = self.final_activation )\n",
    "        \n",
    "        self.edge_update.build(tf.TensorShape([None,self.hparams.path_state_dim]))\n",
    "        self.path_update.build(tf.TensorShape([None,self.hparams.link_state_dim]))\n",
    "        self.readout.build(input_shape = [None,self.hparams.path_state_dim])\n",
    "        self.final.build(input_shape = [None,self.hparams.path_state_dim + self.hparams.readout_units ])\n",
    "\n",
    "        self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "        self.built = True\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        '''\n",
    "\n",
    "        outputs:\n",
    "            Natural parameter\n",
    "        \n",
    "        '''\n",
    "        #this takes features\n",
    "        f_ = inputs\n",
    "\n",
    "        shape = tf.stack([f_[6],self.hparams.link_state_dim-1], axis=0)\n",
    "        #link_state = tf.zeros(shape)\n",
    "        link_state = tf.concat([\n",
    "            tf.expand_dims(f_[1],axis=1),\n",
    "            tf.zeros(shape)\n",
    "        ], axis=1)\n",
    "\n",
    "        shape = tf.stack([f_[7],self.hparams.path_state_dim-1], axis=0)\n",
    "        path_state = tf.concat([\n",
    "            tf.expand_dims(f_[0][0:f_[7]],axis=1),\n",
    "            tf.zeros(shape)\n",
    "        ], axis=1)\n",
    "\n",
    "        links = f_[4]\n",
    "        paths = f_[5]\n",
    "        seqs=  f_[2]\n",
    "        \n",
    "        for _ in range(self.hparams.T):\n",
    "        \n",
    "            h_ = tf.gather(link_state,links)\n",
    "\n",
    "            #TODO move this to feature calculation\n",
    "            ids=tf.stack([paths, seqs], axis=1)            \n",
    "            max_len = tf.reduce_max(seqs)+1\n",
    "            shape = tf.stack([f_[7], max_len, self.hparams.link_state_dim])\n",
    "            lens = tf.segment_sum(data=tf.ones_like(paths),\n",
    "                                    segment_ids=paths)\n",
    "\n",
    "            link_inputs = tf.scatter_nd(ids, h_, shape)\n",
    "            #TODO move to tf.keras.RNN\n",
    "            outputs, path_state = tf.nn.dynamic_rnn(self.path_update,\n",
    "                                                    link_inputs,\n",
    "                                                    sequence_length=lens,\n",
    "                                                    initial_state = path_state,\n",
    "                                                    dtype=tf.float32)\n",
    "            m = tf.gather_nd(outputs,ids)\n",
    "\n",
    "            m = tf.unsorted_segment_sum(m, links ,f_[6])\n",
    "\n",
    "            #Keras cell expects a list\n",
    "            link_state,_ = self.edge_update(m, [link_state])\n",
    "            \n",
    "        if self.hparams.learn_embedding:\n",
    "            r = self.readout(path_state,training=training)\n",
    "            o = self.final(tf.concat([r,path_state], axis=1))\n",
    "            \n",
    "        else:\n",
    "            r = self.readout(tf.stop_gradient(path_state),training=training)\n",
    "            o = self.final(tf.concat([r, tf.stop_gradient(path_state)], axis=1) )\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        features, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loc  = predictions[...,0] \n",
    "            c = np.log(np.expm1( np.float32(0.098) ))\n",
    "            scale = tf.math.softplus(c + predictions[...,1]) + np.float32(1e-9)\n",
    "            delay_prediction = loc\n",
    "            jitter_prediction = scale**2\n",
    "            \n",
    "            x = features\n",
    "            y = labels\n",
    "            n = x[3] - y[2]\n",
    "            _2sigma = np.float32(2.0)*scale**2\n",
    "            nll = n*y[3]/_2sigma + n*tf.math.squared_difference(y[0], loc)/_2sigma + n*tf.math.log(scale)\n",
    "            loss = tf.reduce_sum(nll)/np.float32(1e6)\n",
    "            regularization_loss = sum(self.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.mae_metric.update_state(labels, predictions)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AbishekNet(tf.keras.Model):\n",
    "#     def __init__(self,hparams, output_units=2, final_activation=None):\n",
    "#         super(RouteNet, self).__init__()\n",
    "\n",
    "#         self.hparams = hparams\n",
    "#         self.output_units = output_units\n",
    "#         self.final_activation = final_activation\n",
    "        \n",
    "\n",
    "        \n",
    "#     def build(self, input_shape=None):\n",
    "#         del input_shape\n",
    "\n",
    "#         self.edge_update = tf.keras.layers.GRUCell(self.hparams.link_state_dim, name=\"edge_update\")\n",
    "#         self.path_update = tf.keras.layers.GRUCell(self.hparams.path_state_dim, name=\"path_update\")\n",
    "        \n",
    "        \n",
    "#         self.readout = tf.keras.models.Sequential(name='readout')\n",
    "\n",
    "#         for i in range(self.hparams.readout_layers):\n",
    "#             self.readout.add(tf.keras.layers.Dense(self.hparams.readout_units, \n",
    "#                     activation=tf.nn.selu,\n",
    "#                     kernel_regularizer=tf.contrib.layers.l2_regularizer(self.hparams.l2)))\n",
    "#             self.readout.add(tf.keras.layers.Dropout(rate=self.hparams.dropout_rate))\n",
    "\n",
    "#         self.final = keras.layers.Dense(self.output_units, \n",
    "#                 kernel_regularizer=tf.contrib.layers.l2_regularizer(self.hparams.l2_2),\n",
    "#                 activation = self.final_activation )\n",
    "        \n",
    "#         self.edge_update.build(tf.TensorShape([None,self.hparams.path_state_dim]))\n",
    "#         self.path_update.build(tf.TensorShape([None,self.hparams.link_state_dim]))\n",
    "#         self.readout.build(input_shape = [None,self.hparams.path_state_dim])\n",
    "#         self.final.build(input_shape = [None,self.hparams.path_state_dim + self.hparams.readout_units ])\n",
    "\n",
    "#         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "#         self.built = True\n",
    "    \n",
    "    \n",
    "#     def call(self, inputs, training=False):\n",
    "#         '''\n",
    "\n",
    "#         outputs:\n",
    "#             Natural parameter\n",
    "        \n",
    "#         '''\n",
    "#         #this takes features\n",
    "#         f_ = inputs\n",
    "#         sample = inputs\n",
    "\n",
    "\n",
    "#         n_links = 20\n",
    "\n",
    "#         shape = tf.stack([n_links,self.hparams.link_state_dim-1], axis=0)\n",
    "#         link_state = tf.zeros(shape)\n",
    "#         # link_state = tf.concat([\n",
    "#         #     tf.expand_dims(f_['capacities'],axis=1),\n",
    "#         #     tf.zeros(shape)\n",
    "#         # ], axis=1)\n",
    "\n",
    "#         n_paths = 10\n",
    "#         shape = tf.stack([n_paths,self.hparams.path_state_dim-1], axis=0)\n",
    "#         # path_state = tf.concat([\n",
    "#         #     tf.expand_dims(f_['traffic'][0:f_[\"n_paths\"]],axis=1),\n",
    "#         #     tf.zeros(shape)\n",
    "#         # ], axis=1)\n",
    "#         path_state = tf.zeros(shape)\n",
    "#         links = f_['links']\n",
    "#         paths = f_['paths']\n",
    "#         seqs = f_['sequences']\n",
    "        \n",
    "#         for _ in range(self.hparams.T):\n",
    "        \n",
    "#             h_ = tf.gather(link_state,links)\n",
    "\n",
    "#             #TODO move this to feature calculation\n",
    "#             ids=tf.stack([paths, seqs], axis=1)            \n",
    "#             max_len = tf.reduce_max(seqs)+1\n",
    "\n",
    "#             shape = tf.stack([n_paths, max_len, self.hparams.link_state_dim])\n",
    "#             lens = tf.segment_sum(data=tf.ones_like(paths),\n",
    "#                                     segment_ids=paths)\n",
    "\n",
    "#             link_inputs = tf.scatter_nd(ids, h_, shape)\n",
    "#             #TODO move to tf.keras.RNN\n",
    "#             outputs, path_state = tf.nn.dynamic_rnn(self.path_update,\n",
    "#                                                     link_inputs,\n",
    "#                                                     sequence_length=lens,\n",
    "#                                                     initial_state = path_state,\n",
    "#                                                     dtype=tf.float32)\n",
    "#             m = tf.gather_nd(outputs,ids)\n",
    "#             m = tf.unsorted_segment_sum(m, links ,n_links)\n",
    "\n",
    "#             #Keras cell expects a list\n",
    "#             link_state,_ = self.edge_update(m, [link_state])\n",
    "            \n",
    "#         if self.hparams.learn_embedding:\n",
    "#             r = self.readout(path_state,training=training)\n",
    "#             o = self.final(tf.concat([r,path_state], axis=1))\n",
    "            \n",
    "#         else:\n",
    "#             r = self.readout(tf.stop_gradient(path_state),training=training)\n",
    "#             o = self.final(tf.concat([r, tf.stop_gradient(path_state)], axis=1) )\n",
    "\n",
    "#         return o\n",
    "\n",
    "\n",
    "#     def train_step(self, data):\n",
    "#         features, labels = data\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             predictions = self(features, training=True)\n",
    "#             loc  = predictions[...,0] \n",
    "#             c = np.log(np.expm1( np.float32(0.098) ))\n",
    "#             scale = tf.math.softplus(c + predictions[...,1]) + np.float32(1e-9)\n",
    "#             delay_prediction = loc\n",
    "#             jitter_prediction = scale**2\n",
    "            \n",
    "#             x = features\n",
    "#             y = labels\n",
    "#             packets = y[:, 0]\n",
    "#             drops = y[:,1]\n",
    "#             n = packets-drops\n",
    "#             # n = x['packets']-y['drops']\n",
    "#             _2sigma = np.float32(2.0)*scale**2\n",
    "            \n",
    "#             delay = y[:, 3]\n",
    "#             nll = n*tf.math.squared_difference(delay, loc)/_2sigma + n*tf.math.log(scale)\n",
    "#             # nll = n*y['jitter']/_2sigma + n*tf.math.squared_difference(y['delay'], loc)/_2sigma + n*tf.math.log(scale)\n",
    "            \n",
    "#             loss = tf.reduce_sum(nll)/np.float32(1e6)\n",
    "#             regularization_loss = sum(self.losses)\n",
    "#             total_loss = loss + regularization_loss\n",
    "\n",
    "#         gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "\n",
    "#         self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "#         self.mae_metric(labels, predictions)\n",
    "\n",
    "#         return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_fn(k, val):\n",
    "    '''Scales given feature\n",
    "    Args:\n",
    "        k: key\n",
    "        val: tensor value\n",
    "    '''\n",
    "\n",
    "    if k == 'traffic':\n",
    "        return (val-0.18)/.15\n",
    "    if k == 'capacities':\n",
    "        return val/10.0\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_fn(k, val):\n",
    "#     '''Scales given feature\n",
    "#     Args:\n",
    "#         k: key\n",
    "#         val: tensor value\n",
    "#     '''\n",
    "\n",
    "#     if k == 0:\n",
    "#         return (val-0.18)/.15\n",
    "#     if k == 6:\n",
    "#         return val/10.0\n",
    "\n",
    "#     return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized, target=None, normalize=True):\n",
    "    '''\n",
    "    Target is the name of predicted variable-deprecated\n",
    "    '''\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.name_scope('parse'):    \n",
    "            #TODO add feature spec class\n",
    "            features = tf.io.parse_single_example(\n",
    "                serialized,\n",
    "                features={\n",
    "                    'traffic':tf.VarLenFeature(tf.float32),\n",
    "                    'delay':tf.VarLenFeature(tf.float32),\n",
    "                    'logdelay':tf.VarLenFeature(tf.float32),\n",
    "                    'jitter':tf.VarLenFeature(tf.float32),\n",
    "                    'drops':tf.VarLenFeature(tf.float32),\n",
    "                    'packets':tf.VarLenFeature(tf.float32),\n",
    "                    'capacities':tf.VarLenFeature(tf.float32),\n",
    "                    'links':tf.VarLenFeature(tf.int64),\n",
    "                    'paths':tf.VarLenFeature(tf.int64),\n",
    "                    'sequences':tf.VarLenFeature(tf.int64),\n",
    "                    'n_links':tf.FixedLenFeature([],tf.int64), \n",
    "                    'n_paths':tf.FixedLenFeature([],tf.int64),\n",
    "                    'n_total':tf.FixedLenFeature([],tf.int64)\n",
    "                })\n",
    "            for k in ['traffic','delay','logdelay','jitter','drops','packets','capacities','links','paths','sequences']:\n",
    "                features[k] = tf.sparse.to_dense( features[k] )\n",
    "                if normalize:\n",
    "                    features[k] = scale_fn(k, features[k])\n",
    "\n",
    "\n",
    "    #return {k:v for k,v in features.items() if k is not target },features[target]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(serialized, target=None, normalize=True):\n",
    "#     '''\n",
    "#     Target is the name of predicted variable-deprecated\n",
    "#     '''\n",
    "#     with tf.device(\"/cpu:0\"):\n",
    "#         with tf.name_scope('parse'):    \n",
    "#             #TODO add feature spec class\n",
    "#             features = tf.io.parse_single_example(\n",
    "#                 serialized,\n",
    "#                 features={\n",
    "#                     'traffic':tf.VarLenFeature(tf.float32),#0 traffic\n",
    "#                     'delay':tf.VarLenFeature(tf.float32),#1 delay\n",
    "#                     'logdelay':tf.VarLenFeature(tf.float32),#2 logdelay\n",
    "#                     'jitter':tf.VarLenFeature(tf.float32),#3 jitter\n",
    "#                     'drops':tf.VarLenFeature(tf.float32),#4 drops\n",
    "#                     'packets':tf.VarLenFeature(tf.float32),#5 packets\n",
    "#                     'capacities':tf.VarLenFeature(tf.float32),#6 capacities\n",
    "#                     'links':tf.VarLenFeature(tf.int64),#7 links\n",
    "#                     'paths':tf.VarLenFeature(tf.int64),#8 paths\n",
    "#                     'sequences':tf.VarLenFeature(tf.int64),#9 sequences\n",
    "#                     'n_links':tf.FixedLenFeature([],tf.int64), #10 n_links\n",
    "#                     'n_paths':tf.FixedLenFeature([],tf.int64), #11 n_paths\n",
    "#                     'n_total':tf.FixedLenFeature([],tf.int64) #12 n_total\n",
    "#                 })\n",
    "#             features = [features[f] for f in features.keys]\n",
    "#             for k in [0,1,2,3,4,5,6,7,8,9]:\n",
    "#                 features[k] = tf.sparse.to_dense( features[k] )\n",
    "#                 if normalize:\n",
    "#                     features[k] = scale_fn(k, features[k])\n",
    "\n",
    "\n",
    "#     #return {k:v for k,v in features.items() if k is not target },features[target]\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cummax(alist, extractor):\n",
    "    with tf.name_scope('cummax'):\n",
    "        maxes = [tf.reduce_max( extractor(v) ) + 1 for v in alist ]\n",
    "        cummaxes = [tf.zeros_like(maxes[0])]\n",
    "        for i in range(len(maxes)-1):\n",
    "            cummaxes.append( tf.math.add_n(maxes[0:i+1]))\n",
    "        \n",
    "    return cummaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transformation_func(it, batch_size=32):\n",
    "#     with tf.name_scope(\"transformation_func\"):\n",
    "#         vs = [it.get_next() for _ in range(batch_size)]\n",
    "        \n",
    "#         links_cummax = cummax(vs,lambda v:v['links'] )\n",
    "#         paths_cummax = cummax(vs,lambda v:v['paths'] )\n",
    "        \n",
    "#         tensors = ({\n",
    "#                 'traffic':tf.concat([v['traffic'] for v in vs], axis=0),\n",
    "#                 'capacities': tf.concat([v['capacities'] for v in vs], axis=0),\n",
    "#                 'sequences':tf.concat([v['sequences'] for v in vs], axis=0),\n",
    "#                 'packets':tf.concat([v['packets'] for v in vs], axis=0),\n",
    "#                 'links':tf.concat([v['links'] + m for v,m in zip(vs, links_cummax) ], axis=0),\n",
    "#                 'paths':tf.concat([v['paths'] + m for v,m in zip(vs, paths_cummax) ], axis=0),\n",
    "#                 'n_links':tf.math.add_n([v['n_links'] for v in vs]),\n",
    "#                 'n_paths':tf.math.add_n([v['n_paths'] for v in vs]),\n",
    "#                 'n_total':tf.math.add_n([v['n_total'] for v in vs])\n",
    "#             },   {\n",
    "#                 'delay' : tf.concat([v['delay'] for v in vs], axis=0),\n",
    "#                 'logdelay' : tf.concat([v['logdelay'] for v in vs], axis=0),\n",
    "#                 'drops' : tf.concat([v['drops'] for v in vs], axis=0),\n",
    "#                 'jitter' : tf.concat([v['jitter'] for v in vs], axis=0),\n",
    "#                 }\n",
    "#             )\n",
    "    \n",
    "#     return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transformation_func(it, batch_size=32):\n",
    "#     with tf.name_scope(\"transformation_func\"):\n",
    "#         vs = [it.get_next() for _ in range(batch_size)]\n",
    "        \n",
    "#         links_cummax = cummax(vs,lambda v:v[7] )\n",
    "#         paths_cummax = cummax(vs,lambda v:v[8] )\n",
    "        \n",
    "#         tensors = ([\n",
    "#                 tf.concat([v[0] for v in vs], axis=0),#0 traffic\n",
    "#                 tf.concat([v[6] for v in vs], axis=0),#1 capacities\n",
    "#                 tf.concat([v[9] for v in vs], axis=0),#2 sequences\n",
    "#                 tf.concat([v[5] for v in vs], axis=0),#3 packets\n",
    "#                 tf.concat([v[7] + m for v,m in zip(vs, links_cummax) ], axis=0),#4 links\n",
    "#                 tf.concat([v[8] + m for v,m in zip(vs, paths_cummax) ], axis=0),#5 paths\n",
    "#                 tf.math.add_n([v[10] for v in vs]),#6 n_links\n",
    "#                 tf.math.add_n([v[11] for v in vs]),#7 n_paths\n",
    "#                 tf.math.add_n([v[12] for v in vs])#8 n_total\n",
    "#             ],   [\n",
    "#                 tf.concat([v[1] for v in vs], axis=0),#0 delay\n",
    "#                 tf.concat([v[2] for v in vs], axis=0),#1 logdelay\n",
    "#                 tf.concat([v[4] for v in vs], axis=0),#2 drops\n",
    "#                 tf.concat([v[3] for v in vs], axis=0),#3 jitter\n",
    "#                 ]\n",
    "#             )\n",
    "    \n",
    "#     return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_func(it, batch_size=32):\n",
    "    with tf.name_scope(\"transformation_func\"):\n",
    "        vs = [it.get_next() for _ in range(batch_size)]\n",
    "        \n",
    "        links_cummax = cummax(vs,lambda v:v['links'] )\n",
    "        paths_cummax = cummax(vs,lambda v:v['paths'] )\n",
    "        \n",
    "        tensors = ([\n",
    "                tf.concat([v['traffic'] for v in vs], axis=0),#0 traffic\n",
    "                tf.concat([v['capacities'] for v in vs], axis=0),#1 capacities\n",
    "                tf.concat([v['sequences'] for v in vs], axis=0),#2 sequences\n",
    "                tf.concat([v['packets'] for v in vs], axis=0),#3 packets\n",
    "                tf.concat([v['links'] + m for v,m in zip(vs, links_cummax) ], axis=0),#4 links\n",
    "                tf.concat([v['paths'] + m for v,m in zip(vs, paths_cummax) ], axis=0),#5 paths\n",
    "                tf.math.add_n([v['n_links'] for v in vs]),#6 n_links\n",
    "                tf.math.add_n([v['n_paths'] for v in vs]),#7 n_paths\n",
    "                tf.math.add_n([v['n_total'] for v in vs])#8 n_total\n",
    "            ],   [\n",
    "                tf.concat([v['delay'] for v in vs], axis=0),#0 delay\n",
    "                tf.concat([v['logdelay'] for v in vs], axis=0), #1 logdelay\n",
    "                tf.concat([v['drops'] for v in vs], axis=0),#2 drops\n",
    "                tf.concat([v['jitter'] for v in vs], axis=0),#3 jitter\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_input_fn(filenames,hparams,shuffle_buf=1000, target='delay'):\n",
    "    \n",
    "    files = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    files = files.shuffle(len(filenames))\n",
    "\n",
    "    ds = files.apply(tf.data.experimental.parallel_interleave(\n",
    "        tf.data.TFRecordDataset, cycle_length=4))\n",
    "\n",
    "    if shuffle_buf:\n",
    "        ds = ds.apply(tf.data.experimental.shuffle_and_repeat(shuffle_buf))\n",
    "    else :\n",
    "        # sample 10 % for evaluation because it is time consuming\n",
    "        ds = ds.filter(lambda x: tf.random_uniform(shape=())< 0.1)\n",
    "\n",
    "    ds = ds.map(lambda buf:parse(buf,target), \n",
    "        num_parallel_calls=2)\n",
    "    ds=ds.prefetch(10)\n",
    "\n",
    "    it =ds.make_one_shot_iterator()\n",
    "    sample = transformation_func(it,hparams.batch_size)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0624 12:43:27.897175 140636747016000 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "    node_count=14,\n",
    "    link_state_dim=32, \n",
    "    #[4, 8, 16, 32, 64]\n",
    "    path_state_dim=32,\n",
    "    #[2, 4, 8, 16, 32, 64]\n",
    "    T=8,\n",
    "    readout_units=8,\n",
    "    learning_rate=0.001,\n",
    "    #[.001, .01, .05]\n",
    "    batch_size=16,\n",
    "    #[8, 16, 32, 64]\n",
    "    dropout_rate=0.5,\n",
    "    #[.5] leave\n",
    "    l2=0.1,\n",
    "    #regulirization constants\n",
    "    #[.05, .1, .2]\n",
    "    l2_2=0.01,\n",
    "    #[.005, .01, .02]\n",
    "    learn_embedding=True, # If false, only the readout is trained\n",
    "    readout_layers=2, # number of hidden layers in readout model\n",
    "    #[2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 12:43:27.969984 140636747016000 deprecation.py:323] From <ipython-input-22-8f4c204d6b8c>:7: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0624 12:43:27.983568 140636747016000 deprecation.py:323] From <ipython-input-22-8f4c204d6b8c>:10: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W0624 12:43:28.008671 140636747016000 deprecation.py:323] From <ipython-input-22-8f4c204d6b8c>:19: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "testinput = tfrecord_input_fn(train_data, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"route_net_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "edge_update (GRUCell)        multiple                  6240      \n",
      "_________________________________________________________________\n",
      "path_update (GRUCell)        multiple                  6240      \n",
      "_________________________________________________________________\n",
      "readout (Sequential)         multiple                  336       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  82        \n",
      "_________________________________________________________________\n",
      "mae (MeanAbsoluteError)      multiple                  2         \n",
      "=================================================================\n",
      "Total params: 12,900\n",
      "Trainable params: 12,898\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_r = RouteNet(hparams, output_units=2)\n",
    "_r.build(input_shape=(8,))\n",
    "_r.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "_r.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 13:04:09.044159 140636747016000 optimizer_v2.py:979] Gradients does not exist for variables ['kernel_15:0', 'recurrent_kernel_10:0', 'bias_15:0', 'kernel_16:0', 'recurrent_kernel_11:0', 'bias_16:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Identity_5:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_r.train_step(testinput)['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<tf.Tensor 'transformation_func/concat:0' shape=(?,) dtype=float32>,\n",
       "       <tf.Tensor 'transformation_func/concat_1:0' shape=(?,) dtype=float32>,\n",
       "       <tf.Tensor 'transformation_func/concat_2:0' shape=(?,) dtype=int64>,\n",
       "       <tf.Tensor 'transformation_func/concat_3:0' shape=(?,) dtype=float32>,\n",
       "       <tf.Tensor 'transformation_func/concat_4:0' shape=(?,) dtype=int64>,\n",
       "       <tf.Tensor 'transformation_func/concat_5:0' shape=(?,) dtype=int64>,\n",
       "       <tf.Tensor 'transformation_func/AddN:0' shape=() dtype=int64>,\n",
       "       <tf.Tensor 'transformation_func/AddN_1:0' shape=() dtype=int64>,\n",
       "       <tf.Tensor 'transformation_func/AddN_2:0' shape=() dtype=int64>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(testinput[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model input: expected no data, but got:', ([<tf.Tensor 'transformation_func/concat:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_1:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_2:0' shape=(?,) dtype=int64>, <tf.Tensor 'transformation_func/concat_3:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_4:0' shape=(?,) dtype=int64>, <tf.Tensor 'transformation_func/concat_5:0' shape=(?,) dtype=int64>, <tf.Tensor 'transformation_func/AddN:0' shape=() dtype=int64>, <tf.Tensor 'transformation_func/AddN_1:0' shape=() dtype=int64>, <tf.Tensor 'transformation_func/AddN_2:0' shape=() dtype=int64>], [<tf.Tensor 'transformation_func/concat_6:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_7:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_8:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_9:0' shape=(?,) dtype=float32>]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-0005789d9bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    303\u001b[0m       raise ValueError(\n\u001b[1;32m    304\u001b[0m           \u001b[0;34m'Error when checking model '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m           'expected no data, but got:', data)\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Error when checking model input: expected no data, but got:', ([<tf.Tensor 'transformation_func/concat:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_1:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_2:0' shape=(?,) dtype=int64>, <tf.Tensor 'transformation_func/concat_3:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_4:0' shape=(?,) dtype=int64>, <tf.Tensor 'transformation_func/concat_5:0' shape=(?,) dtype=int64>, <tf.Tensor 'transformation_func/AddN:0' shape=() dtype=int64>, <tf.Tensor 'transformation_func/AddN_1:0' shape=() dtype=int64>, <tf.Tensor 'transformation_func/AddN_2:0' shape=() dtype=int64>], [<tf.Tensor 'transformation_func/concat_6:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_7:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_8:0' shape=(?,) dtype=float32>, <tf.Tensor 'transformation_func/concat_9:0' shape=(?,) dtype=float32>]))"
     ]
    }
   ],
   "source": [
    "_r.fit(testinput, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
